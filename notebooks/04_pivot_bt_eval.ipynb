{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70df8c1",
   "metadata": {},
   "source": [
    "# 04 — Pivot (Tagalog→Waray→Cebuano) and Back-translation\n",
    "\n",
    "Purpose: Investigate semi-supervised and data-augmentation approaches such as pivot translation and back-translation to enhance training data for low-resource languages, enabling more effective cross-lingual transfer and translation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f3130",
   "metadata": {},
   "source": [
    "## Pivot Translation\n",
    "\n",
    "- Translates source → intermediate (pivot) → target, or here Tagalog → Waray → Cebuano.\n",
    "- Evaluates if pivoting improves or hurts quality.\n",
    "\n",
    "**Files created:**\n",
    "\n",
    "- `test.pivot` - Waray pivot translations of test.src (Tagalog → Waray).\n",
    "- `hyp.pivot2tgt` - Cebuano translations of test.pivot (Waray → Cebuano).\n",
    "- `metrics.json` with new scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d68a39",
   "metadata": {},
   "source": [
    "### Pivot setup: extract test source/ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9c7d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared: ..\\experiments\\pivot\\test.src and ..\\experiments\\pivot\\test.ref lines: 2750\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "pivot_model = \"facebook/nllb-200-distilled-600M\"\n",
    "root = Path(\"..\") # project root (one level up from notebooks/)\n",
    "pp   = root / \"data\" / \"processed\"\n",
    "exp  = root / \"experiments\" / \"pivot\"\n",
    "exp.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load test TSV using pandas' interpretation\n",
    "df = pd.read_csv(pp / \"test.tsv\", sep=\"\\t\", header=None, names=[\"src\",\"tgt\"])\n",
    "\n",
    "# Clean target: remove embedded newlines, excess whitespace\n",
    "df[\"tgt\"] = (\n",
    "    df[\"tgt\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Write clean src and tgt files\n",
    "df[\"src\"].to_csv(exp / \"test.src\", index=False, header=False)\n",
    "df[\"tgt\"].to_csv(exp / \"test.ref\", index=False, header=False)\n",
    "\n",
    "print(\"Prepared:\", exp / \"test.src\", \"and\", exp / \"test.ref\", \"lines:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69038a",
   "metadata": {},
   "source": [
    "This cell:\n",
    "- defines the pivot model (`facebook/nllb-200-distilled-600M`) and creates `experiments/pivot/`\n",
    "- loads `data/processed/test.tsv` (columns: `src`, `tgt`)\n",
    "- writes:\n",
    "  - `experiments/pivot/test.src`  → the Tagalog test sources (one per line)\n",
    "  - `experiments/pivot/test.ref`  → the Cebuano references (one per line)\n",
    "\n",
    "These files are the inputs/ground truth for the pivot experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea754cd8",
   "metadata": {},
   "source": [
    "### Source → Pivot translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f93e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2,750 lines from ..\\experiments\\pivot\\test.src\n",
      "Loading model: facebook/nllb-200-distilled-600M\n",
      "Using source=tgl_Latn tag='tgl_Latn' → target=war_Latn (id=256194)\n",
      "Device: cuda | batch=12 | beams=2\n",
      "✅ Wrote 2750 translations → ..\\experiments\\pivot\\test.pivot\n"
     ]
    }
   ],
   "source": [
    "!python ../src/decode/translate_simple.py \\\n",
    "  --model $pivot_model \\\n",
    "  --src ../experiments/pivot/test.src \\\n",
    "  --out ../experiments/pivot/test.pivot \\\n",
    "  --src_code tgl_Latn \\\n",
    "  --tgt_code war_Latn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f4dc1",
   "metadata": {},
   "source": [
    "This runs the batch translator on `test.src` and produces `test.pivot`.\n",
    "\n",
    "- Input: `experiments/pivot/test.src` (Tagalog)\n",
    "- Model: `facebook/nllb-200-distilled-600M`\n",
    "- Language codes used internally: `tgl_Latn` → `war_Latn`\n",
    "- Output: `experiments/pivot/test.pivot` (Waray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef3556",
   "metadata": {},
   "source": [
    "### Pivot → Target translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8dfdbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2,750 lines from ..\\experiments\\pivot\\test.pivot\n",
      "Loading model: facebook/nllb-200-distilled-600M\n",
      "Using source=war_Latn tag='war_Latn' → target=ceb_Latn (id=256035)\n",
      "Device: cuda | batch=12 | beams=2\n",
      "✅ Wrote 2750 translations → ..\\experiments\\pivot\\hyp.pivot2tgt\n"
     ]
    }
   ],
   "source": [
    "!python ../src/decode/translate_simple.py \\\n",
    "  --model $pivot_model \\\n",
    "  --src ../experiments/pivot/test.pivot \\\n",
    "  --out ../experiments/pivot/hyp.pivot2tgt \\\n",
    "  --src_code war_Latn \\\n",
    "  --tgt_code ceb_Latn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728383b",
   "metadata": {},
   "source": [
    "This translates the pivot text into the final target and writes `hyp.pivot2tgt`.\n",
    "\n",
    "- Input: `experiments/pivot/test.pivot`\n",
    "- Model: `facebook/nllb-200-distilled-600M`\n",
    "- Language codes (current config): `war_Latn` → `ceb_Latn`\n",
    "- Output: `experiments/pivot/hyp.pivot2tgt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c710e481",
   "metadata": {},
   "source": [
    "### Score the pivot system (BLEU & chrF2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5debf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"BLEU\": 20.58,\n",
      "  \"chrF2\": 42.37,\n",
      "  \"ref_len\": 114606,\n",
      "  \"sys_len\": 84981,\n",
      "  \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.5.1\",\n",
      "  \"sacrebleu_version\": \"2.5.1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python ../src/eval/score.py \\\n",
    "  --ref ../experiments/pivot/test.ref \\\n",
    "  --hyp ../experiments/pivot/hyp.pivot2tgt \\\n",
    "  --out ../experiments/pivot/metrics.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75883ab4",
   "metadata": {},
   "source": [
    "This evaluates the pivot pipeline output against the gold references.\n",
    "\n",
    "- Reference: `experiments/pivot/test.ref` (gold Tagalog)\n",
    "- Hypothesis: `experiments/pivot/hyp.pivot2tgt` (pivot pipeline output)\n",
    "- Output metrics JSON: `experiments/pivot/metrics.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd8ae4",
   "metadata": {},
   "source": [
    "## Back-translation\n",
    "\n",
    "- Uses *monolingual target text* to synthesize extra training pairs:\n",
    "  - Cebuano mono text → translate backward to Tagalog.\n",
    "  - Creates synthetic pairs `(Tagalog_bt, Cebuano_real)`.\n",
    "- Merges them with your real training data (`train_plus_bt.tsv`).\n",
    "- The next fine-tune round can use this augmented data for better fluency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e2d25",
   "metadata": {},
   "source": [
    "### Mine Cebuano Monolingual Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b3a4e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote 20,000 lines → D:\\OneDrive\\Documents\\My Learning Resource\\University Courses\\DLSU\\2025-26\\T1\\CSC715M\\assignments\\mc02\\data\\mono\\target\\mono.txt\n",
      "   Source lines: 27,227 | After dedupe: 27,158 | After limit: 20,000\n",
      "   Length filter: 6–240 chars | Seed: 42\n"
     ]
    }
   ],
   "source": [
    "!python ../src/data/mine_mono.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211005a",
   "metadata": {},
   "source": [
    "This step extracts monolingual Cebuano sentences from the existing parallel training and development sets.  \n",
    "It will be used for back-translation (BT) to create synthetic parallel data later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e08efe4",
   "metadata": {},
   "source": [
    "### Translate monolingual target → source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848e776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20,000 lines from ..\\data\\mono\\target\\mono.txt\n",
      "Loading model: facebook/nllb-200-distilled-600M\n",
      "Using source=ceb_Latn tag='ceb_Latn' → target=tgl_Latn (id=256174)\n",
      "Device: cuda | batch=12 | beams=2\n",
      "✅ Wrote 20000 translations → ..\\data\\mono\\target\\mono.bt.src\n"
     ]
    }
   ],
   "source": [
    "# Translate monolingual Cebuano -> Tagalog for back-translation\n",
    "!python ../src/decode/translate_simple.py \\\n",
    "  --model $pivot_model \\\n",
    "  --src ../data/mono/target/mono.txt \\\n",
    "  --out ../data/mono/target/mono.bt.src \\\n",
    "  --src_code ceb_Latn \\\n",
    "  --tgt_code tgl_Latn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd7eafb",
   "metadata": {},
   "source": [
    "- Input (you provide): `data/mono/target/mono.txt` (Cebuano lines, one per line)  \n",
    "- Translation output: `data/mono/target/mono.bt.src` (synthetic Tagalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3648bc95",
   "metadata": {},
   "source": [
    "### Pair synthetic source with original target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "330775d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote synthetic pairs: 20000\n"
     ]
    }
   ],
   "source": [
    "# Join into synthetic TSV\n",
    "mono_tgt = [l.strip() for l in open(\"../data/mono/target/mono.txt\", encoding=\"utf-8\") if l.strip()]\n",
    "mono_src = [l.strip() for l in open(\"../data/mono/target/mono.bt.src\", encoding=\"utf-8\") if l.strip()]\n",
    "N = min(len(mono_tgt), len(mono_src))\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "synth = pd.DataFrame({\"src\": mono_src[:N], \"tgt\": mono_tgt[:N]})\n",
    "synth.to_csv(\"../data/processed/synth_bt.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"Wrote synthetic pairs:\", len(synth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d256c138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged training size: 42851\n"
     ]
    }
   ],
   "source": [
    "# Merge with real train.tsv\n",
    "train = pd.read_csv(\"../data/processed/train.tsv\", sep=\"\\t\", header=None, names=[\"src\",\"tgt\"])\n",
    "merged = pd.concat([train, synth]).sample(frac=1.0, random_state=42)\n",
    "merged.to_csv(\"../data/processed/train_plus_bt.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "print(\"Merged training size:\", len(merged))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef8edf1",
   "metadata": {},
   "source": [
    "\n",
    "- Creates `data/processed/synth_bt.tsv` with columns: `src` (synthetic), `tgt` (original mono Cebuano)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0dfee",
   "metadata": {},
   "source": [
    "### Next step\n",
    "- Re-run fine-tuning (in your fine-tune notebook) pointing to `data/processed/train_plus_bt.tsv`  \n",
    "  and compare metrics vs. the baseline fine-tune."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
